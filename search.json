[
  {
    "objectID": "calrissian-cwl.html",
    "href": "calrissian-cwl.html",
    "title": "openEO Geotrellis documentation",
    "section": "",
    "text": "# CWL based openEO processing using Calrissian\nThis document describes how CWL based processing is implemented in the openEO GeoPySpark driver when running on a Kubernetes cluster."
  },
  {
    "objectID": "calrissian-cwl.html#background-and-terminology",
    "href": "calrissian-cwl.html#background-and-terminology",
    "title": "openEO Geotrellis documentation",
    "section": "Background and terminology",
    "text": "Background and terminology\nCWL (Common Workflow Language) is an open standard for describing how to run (command line) tools and connect them to create workflows. While CWL and openEO are a bit competing due to this conceptual overlap, there is demand to run existing or new CWL workflows as part of a larger openEO processing chain. This is comparable to UDFs which allow to run (Python) scripts as part of an openEO workflow.\nCalrissian is a CWL implementation designed to run inside a Kubernetes cluster."
  },
  {
    "objectID": "calrissian-cwl.html#kubernetes-setup",
    "href": "calrissian-cwl.html#kubernetes-setup",
    "title": "openEO Geotrellis documentation",
    "section": "Kubernetes setup",
    "text": "Kubernetes setup\nNote: this is an extension to the Kubernetes setup discussed in the Calrissian cluster configuration docs.\n\nNamespace\nCreate a namespace for the Calrissian service:\nNAMESPACE_NAME=calrissian-demo-project\nkubectl create namespace \"$NAMESPACE_NAME\"\n\n\nRoles and permissions\nCreate roles:\nkubectl \\\n    --namespace=\"$NAMESPACE\" \\\n    create role \\\n    pod-manager-role \\\n    --verb=create,patch,delete,list,watch \\\n    --resource=pods\n\nkubectl \\\n    --namespace=\"$NAMESPACE_NAME\" \\\n    create role \\\n    log-reader-role \\\n    --verb=get,list \\\n    --resource=pods/log\n\nkubectl \\\n    --namespace=\"$NAMESPACE_NAME\" \\\n    create role \\\n    job-manager-role \\\n    --verb=create,list,get,delete \\\n    --resource=jobs\n\nkubectl \\\n    --namespace=\"$NAMESPACE_NAME\" \\\n    create role \\\n    pvc-reader-role \\\n    --verb=list,get \\\n    --resource=persistentvolumeclaims\nAnd attach roles to the service account in that namespace:\nkubectl \\\n    --namespace=\"$NAMESPACE_NAME\" \\\n    create rolebinding \\\n    pod-manager-default-binding \\\n    --role=pod-manager-role \\\n    --serviceaccount=${NAMESPACE_NAME}:default\n\nkubectl \\\n    --namespace=\"$NAMESPACE_NAME\" \\\n    create rolebinding \\\n    log-reader-default-binding \\\n    --role=log-reader-role \\\n    --serviceaccount=${NAMESPACE_NAME}:default\nAlso attach roles to the appropriate service accounts (in other namespaces)\nENV=dev\n# ENV=staging\n# ENV=prod\nOTHER_NAMESPACE_NAME=spark-jobs-$ENV\n\nfor SERVICE_ACCOUNT_NAME in openeo batch-jobs\ndo\n    kubectl \\\n        --namespace=\"$NAMESPACE_NAME\" \\\n        create rolebinding \\\n        job-manager-${OTHER_NAMESPACE_NAME}-${SERVICE_ACCOUNT_NAME} \\\n        --role=job-manager-role \\\n        --serviceaccount=${OTHER_NAMESPACE_NAME}:${SERVICE_ACCOUNT_NAME}\n\n    kubectl \\\n        --namespace=\"$NAMESPACE_NAME\" \\\n        create rolebinding \\\n        pvc-reader-${OTHER_NAMESPACE_NAME}-${SERVICE_ACCOUNT_NAME} \\\n        --role=pvc-reader-role \\\n        --serviceaccount=${OTHER_NAMESPACE_NAME}:${SERVICE_ACCOUNT_NAME}\ndone\n\n\nStorage\nCreate a StorageClass csi-s3-calrissian for the volumes used by Calrissian. For example with geesefs from ru.yandex.s3.csi:\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: csi-s3-calrissian\nprovisioner: ru.yandex.s3.csi\nparameters:\n  mounter: geesefs\n  # you can set mount options here, for example limit memory cache size (recommended)\n  options: \"--memory-limit 1000 --dir-mode 0777 --file-mode 0666 --no-systemd\"\n  # to use an existing bucket, specify it here:\n  bucket: calrissian\n  csi.storage.k8s.io/provisioner-secret-name: csi-s3-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: csi-s3\n  csi.storage.k8s.io/controller-publish-secret-name: csi-s3-secret\n  csi.storage.k8s.io/controller-publish-secret-namespace: csi-s3\n  csi.storage.k8s.io/node-stage-secret-name: csi-s3-secret\n  csi.storage.k8s.io/node-stage-secret-namespace: csi-s3\n  csi.storage.k8s.io/node-publish-secret-name: csi-s3-secret\n  csi.storage.k8s.io/node-publish-secret-namespace: csi-s3\nCreate the PersistentVolumeClaims for the input, tmp and output volumes. For example, with namespace calrissian-demo-project and storage class csi-s3-calrissian as defined above:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: calrissian-input-data\n  namespace: calrissian-demo-project\nspec:\n  accessModes:\n    - ReadWriteOnce\n    - ReadOnlyMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: csi-s3-calrissian\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: calrissian-tmpout\n  namespace: calrissian-demo-project\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: csi-s3-calrissian\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: calrissian-output-data\n  namespace: calrissian-demo-project\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: csi-s3-calrissian"
  },
  {
    "objectID": "calrissian-cwl.html#further-openeo-geopyspark-driver-configuration",
    "href": "calrissian-cwl.html#further-openeo-geopyspark-driver-configuration",
    "title": "openEO Geotrellis documentation",
    "section": "Further openeo-geopyspark-driver configuration",
    "text": "Further openeo-geopyspark-driver configuration\nFurther configuration of the openeo-geopyspark-driver application is done through the calrissian_config field of GpsBackendConfig (also see the general configuration docs). This field expects (unless no Calrissian integration is necessary) a CalrissianConfig sub-configuration object (defined at openeogeotrellis.config.integrations.calrissian_config), which allows to configure various aspects of the Calrissian integration."
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "The openEO GeoPySpark backend grew organically from a single instance proof of concept to a component that is used in different deployments. As a result, the (need for) configuration options also grew organically, and the current implementation uses a mix of configuration approaches of varying flexibility:\n\nJust hardcoded values directly in the code that needs it. Obviously, this is all but flexible and needs to be avoided.\nGetting a configuration parameter from environment variables directly in the code that needs it (e.g. with os.environ.get() in Python). This allows for some basic flexibility, but the configuration option is hidden deep in the code and cumbersome to maintain (duplication, management of fallback values, …)\nThe ConfirParams class (in openeogeotrellis/configparams.py) was the first approach to have a centralized place for configuration parameters and improve the maintenance and overview challenges.\n\nConfirParams is mostly still based on reading and parsing environment variables. This is fine for basic things like the URL of some service or database, but it doesn’t scale very well:\n\nthe environment variables tend to be sprinkled all over the place: shell scripts, Python startup scripts, Jenkinsfiles, Dockerfiles, etc., possibly across different repositories. It’s hard to figure out where to find, add or update a particular setting.\nenvironment variables only work well for string parameters, but some configuration requires more complex constructs (lists, mappings, nested constructs) and trying to properly encode these in a single string as env variable is not always trivial (e.g. it must survive possible string manipulation in shell scripts along the way).\n\n\n\n\nTo address the limitations of doing configuration through environment variables, (yet) another configuration system was introduced (under #285) based on configuration files. Python was chosen as configuration file format (instead of JSON, YAML, TOML, …) for maximum flexibility and easy integration with the openeo-geopyspark-driver and related code (e.g. type hints and code navigation features). Configuration parameters can be defined directly as Python values, not only the usual suspects like lists, dicts, tuples, sets, etc. but also custom objects or constructs. It also allows to derive parameters dynamically, for example based on environment variables or other contextual information.\nNote that this file based configuration system is introduced gradually and the original ConfirParams is largely kept untouched.\n\n\n\n\n\nA config file with this system is expected to define a config variable at top-level, and it must be an instance of the class openeogeotrellis.config.GpsBackendConfig. A minimal config file looks like this:\nfrom openeogeotrellis.config import GpsBackendConfig\nconfig = GpsBackendConfig()\nWhen starting the openEO GeoPyspark backend, the environment variable OPENEO_BACKEND_CONFIG must be set contain the path to this config file.\n\nAlso look at the default config file which is loaded when OPENEO_BACKEND_CONFIG is not set.\n\n\n\nThe GpsBackendConfig class is defined through the popular attrs package to compactly define attributes with type hints and default values.\nFor example:\nfrom openeo_driver.users.oidc import OidcProvider\n\n@attrs.define\nclass GpsBackendConfig:\n    oidc_providers: List[OidcProvider] = attrs.Factory(list)\nConfig parameter oidc_providers must be a list of OidcProvider instances, and will be an empty list by default. A custom config file that sets this parameter might look like this:\nfrom openeo_driver.users.oidc import OidcProvider\nfrom openeogeotrellis.config import GpsBackendConfig\n\nconfig = GpsBackendConfig(\n    oidc_providers=[\n      OidcProvider(id=\"OIDC\", issuer=\"https://oidc.test\", title=\"test OIDC\"),\n    ],\n)"
  },
  {
    "objectID": "configuration.html#background",
    "href": "configuration.html#background",
    "title": "Configuration",
    "section": "",
    "text": "The openEO GeoPySpark backend grew organically from a single instance proof of concept to a component that is used in different deployments. As a result, the (need for) configuration options also grew organically, and the current implementation uses a mix of configuration approaches of varying flexibility:\n\nJust hardcoded values directly in the code that needs it. Obviously, this is all but flexible and needs to be avoided.\nGetting a configuration parameter from environment variables directly in the code that needs it (e.g. with os.environ.get() in Python). This allows for some basic flexibility, but the configuration option is hidden deep in the code and cumbersome to maintain (duplication, management of fallback values, …)\nThe ConfirParams class (in openeogeotrellis/configparams.py) was the first approach to have a centralized place for configuration parameters and improve the maintenance and overview challenges.\n\nConfirParams is mostly still based on reading and parsing environment variables. This is fine for basic things like the URL of some service or database, but it doesn’t scale very well:\n\nthe environment variables tend to be sprinkled all over the place: shell scripts, Python startup scripts, Jenkinsfiles, Dockerfiles, etc., possibly across different repositories. It’s hard to figure out where to find, add or update a particular setting.\nenvironment variables only work well for string parameters, but some configuration requires more complex constructs (lists, mappings, nested constructs) and trying to properly encode these in a single string as env variable is not always trivial (e.g. it must survive possible string manipulation in shell scripts along the way)."
  },
  {
    "objectID": "configuration.html#config-files",
    "href": "configuration.html#config-files",
    "title": "Configuration",
    "section": "",
    "text": "To address the limitations of doing configuration through environment variables, (yet) another configuration system was introduced (under #285) based on configuration files. Python was chosen as configuration file format (instead of JSON, YAML, TOML, …) for maximum flexibility and easy integration with the openeo-geopyspark-driver and related code (e.g. type hints and code navigation features). Configuration parameters can be defined directly as Python values, not only the usual suspects like lists, dicts, tuples, sets, etc. but also custom objects or constructs. It also allows to derive parameters dynamically, for example based on environment variables or other contextual information.\nNote that this file based configuration system is introduced gradually and the original ConfirParams is largely kept untouched.\n\n\n\n\n\nA config file with this system is expected to define a config variable at top-level, and it must be an instance of the class openeogeotrellis.config.GpsBackendConfig. A minimal config file looks like this:\nfrom openeogeotrellis.config import GpsBackendConfig\nconfig = GpsBackendConfig()\nWhen starting the openEO GeoPyspark backend, the environment variable OPENEO_BACKEND_CONFIG must be set contain the path to this config file.\n\nAlso look at the default config file which is loaded when OPENEO_BACKEND_CONFIG is not set.\n\n\n\nThe GpsBackendConfig class is defined through the popular attrs package to compactly define attributes with type hints and default values.\nFor example:\nfrom openeo_driver.users.oidc import OidcProvider\n\n@attrs.define\nclass GpsBackendConfig:\n    oidc_providers: List[OidcProvider] = attrs.Factory(list)\nConfig parameter oidc_providers must be a list of OidcProvider instances, and will be an empty list by default. A custom config file that sets this parameter might look like this:\nfrom openeo_driver.users.oidc import OidcProvider\nfrom openeogeotrellis.config import GpsBackendConfig\n\nconfig = GpsBackendConfig(\n    oidc_providers=[\n      OidcProvider(id=\"OIDC\", issuer=\"https://oidc.test\", title=\"test OIDC\"),\n    ],\n)"
  },
  {
    "objectID": "layercatalog.html",
    "href": "layercatalog.html",
    "title": "Layer Catalog Configuration",
    "section": "",
    "text": "Data processing in openEO starts from loading EO data. Originally, openEO just provided the load_collection process to load EO data sets, called collections in the openEO API, which are “predefined” by the openEO backend provider (e.g. listed and documented under the GET /collections endpoint). Later, the load_stac process was added, with the ambitious aim to load data from any static STAC catalog or a STAC API Collection, without the requirement that they have to be known in advance by the openEO backend provider.\nRegardless of the value of load_stac, a backend probably still wants to expose specific EO data sets as openEO collections, to make them easily discoverable and straightforward to use.\n\n\n\nThe openEO GeoPySpark driver allows to configure the available collections through a “layer catalog” configuration file as follows:\n\nCreate a JSON file, e.g. layercatalog.json to define the collections (see lower). If desired, it is also possible to work with multiple files, which will be merged automatically.\nPoint to the layer configuration file(s), in one of the following ways:\n\nSet the environment variable OPENEO_CATALOG_FILES to the path of this file (or comma separated sequence of paths to multiple files).\nSet the layer_catalog_files config field in your GpsBackendConfig configuration object to a list of (absolute) paths to the layer catalog files.\n\nBy default, layercatalog.json in the current working directory is assumed, but it is recommended to use absolute paths to avoid any confusion.\n\n\n\nThe basic structure of the layercatalog.json file is an array of JSON objects, each representing a collection, roughly following the STAC Collection schema:\n[\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"EXAMPLE_STAC_CATALOG\",\n    \"title\": \"example_stac_catalog.\",\n    ...\n  },\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"SENTINEL2_L2A\",\n    \"title\": \"Sentinel2 (L2A)\",\n    ...\n  },\n  ...\n]\nAlso see the layercatalog.json used in the unit tests for more inspiration.\n\n\n\nThe openEO GeoPySpark driver supports different types of data sources, each with its own configuration options. The data source type and its configuration options are specified under nested fields “_vito” &gt; “data_source” in the collection object, e.g.:\n[\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"SENTINEL2_L2A\",\n    \"_vito\": {\n      \"data_source\": {\n        \"type\": \"file-s2\",\n        \"opensearch_collection_id\": \"S2\",\n        ...\n\n\n\n\nThe easiest data source type is the “stac” type, which just requires a URL pointing to a STAC catalog or collection. For example, using the example_stac_catalog included in the openeo-geopyspark-driver repository:\n[\n  {\n    \"id\": \"EXAMPLE_STAC_CATALOG\",\n    \"experimental\": true,\n    \"title\": \"example_stac_catalog.\",\n    \"description\": \"Simple Sentinel-2 based stac catalog that is hosted on Github.\",\n    \"license\": \"unknown\",\n    \"summaries\": {\n      \"eo:bands\": [\n        {\n          \"name\": \"B04\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        }\n      ]\n    },\n    \"_vito\": {\n      \"data_source\": {\n        \"type\": \"stac\",\n        \"url\": \"https://raw.githubusercontent.com/Open-EO/openeo-geopyspark-driver/refs/heads/master/docker/local_batch_job/example_stac_catalog/collection.json\"\n      }\n    }\n  }\n]\n\n\n\nTODO\n\n“file-s2”,\n“file-s3”,\n“file-s1-coherence”\n“file-agera5”\n“file-cgls2”\n“file-globspatialonly”\n“file-oscars”\n“file-probav”\n“sentinel-hub”\n“creodias-s1-backscatter”"
  },
  {
    "objectID": "layercatalog.html#background",
    "href": "layercatalog.html#background",
    "title": "Layer Catalog Configuration",
    "section": "",
    "text": "Data processing in openEO starts from loading EO data. Originally, openEO just provided the load_collection process to load EO data sets, called collections in the openEO API, which are “predefined” by the openEO backend provider (e.g. listed and documented under the GET /collections endpoint). Later, the load_stac process was added, with the ambitious aim to load data from any static STAC catalog or a STAC API Collection, without the requirement that they have to be known in advance by the openEO backend provider.\nRegardless of the value of load_stac, a backend probably still wants to expose specific EO data sets as openEO collections, to make them easily discoverable and straightforward to use."
  },
  {
    "objectID": "layercatalog.html#layercatalog.json-configuration",
    "href": "layercatalog.html#layercatalog.json-configuration",
    "title": "Layer Catalog Configuration",
    "section": "",
    "text": "The openEO GeoPySpark driver allows to configure the available collections through a “layer catalog” configuration file as follows:\n\nCreate a JSON file, e.g. layercatalog.json to define the collections (see lower). If desired, it is also possible to work with multiple files, which will be merged automatically.\nPoint to the layer configuration file(s), in one of the following ways:\n\nSet the environment variable OPENEO_CATALOG_FILES to the path of this file (or comma separated sequence of paths to multiple files).\nSet the layer_catalog_files config field in your GpsBackendConfig configuration object to a list of (absolute) paths to the layer catalog files.\n\nBy default, layercatalog.json in the current working directory is assumed, but it is recommended to use absolute paths to avoid any confusion.\n\n\n\nThe basic structure of the layercatalog.json file is an array of JSON objects, each representing a collection, roughly following the STAC Collection schema:\n[\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"EXAMPLE_STAC_CATALOG\",\n    \"title\": \"example_stac_catalog.\",\n    ...\n  },\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"SENTINEL2_L2A\",\n    \"title\": \"Sentinel2 (L2A)\",\n    ...\n  },\n  ...\n]\nAlso see the layercatalog.json used in the unit tests for more inspiration.\n\n\n\nThe openEO GeoPySpark driver supports different types of data sources, each with its own configuration options. The data source type and its configuration options are specified under nested fields “_vito” &gt; “data_source” in the collection object, e.g.:\n[\n  {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Collection\",\n    \"id\": \"SENTINEL2_L2A\",\n    \"_vito\": {\n      \"data_source\": {\n        \"type\": \"file-s2\",\n        \"opensearch_collection_id\": \"S2\",\n        ..."
  },
  {
    "objectID": "layercatalog.html#minimal-stac-based-example",
    "href": "layercatalog.html#minimal-stac-based-example",
    "title": "Layer Catalog Configuration",
    "section": "",
    "text": "The easiest data source type is the “stac” type, which just requires a URL pointing to a STAC catalog or collection. For example, using the example_stac_catalog included in the openeo-geopyspark-driver repository:\n[\n  {\n    \"id\": \"EXAMPLE_STAC_CATALOG\",\n    \"experimental\": true,\n    \"title\": \"example_stac_catalog.\",\n    \"description\": \"Simple Sentinel-2 based stac catalog that is hosted on Github.\",\n    \"license\": \"unknown\",\n    \"summaries\": {\n      \"eo:bands\": [\n        {\n          \"name\": \"B04\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": null,\n          \"wavelength_um\": null,\n          \"aliases\": null,\n          \"gsd\": null\n        }\n      ]\n    },\n    \"_vito\": {\n      \"data_source\": {\n        \"type\": \"stac\",\n        \"url\": \"https://raw.githubusercontent.com/Open-EO/openeo-geopyspark-driver/refs/heads/master/docker/local_batch_job/example_stac_catalog/collection.json\"\n      }\n    }\n  }\n]"
  },
  {
    "objectID": "layercatalog.html#more-advanced-data-source-types",
    "href": "layercatalog.html#more-advanced-data-source-types",
    "title": "Layer Catalog Configuration",
    "section": "",
    "text": "TODO\n\n“file-s2”,\n“file-s3”,\n“file-s1-coherence”\n“file-agera5”\n“file-cgls2”\n“file-globspatialonly”\n“file-oscars”\n“file-probav”\n“sentinel-hub”\n“creodias-s1-backscatter”"
  },
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "openEO GeoPySpark Driver requirements and dependencies",
    "section": "",
    "text": "Python version: at least 3.8\n\n\nThis backend has been tested with: - Something that runs Spark: Kubernetes or YARN (Hadoop), standalone or on your laptop - Accumulo as the tile storage backend for Geotrellis - Reading GeoTiff files directly from disk or object storage"
  },
  {
    "objectID": "requirements.html#operating-environment-dependencies",
    "href": "requirements.html#operating-environment-dependencies",
    "title": "openEO GeoPySpark Driver requirements and dependencies",
    "section": "",
    "text": "This backend has been tested with: - Something that runs Spark: Kubernetes or YARN (Hadoop), standalone or on your laptop - Accumulo as the tile storage backend for Geotrellis - Reading GeoTiff files directly from disk or object storage"
  },
  {
    "objectID": "performance_debug.html",
    "href": "performance_debug.html",
    "title": "Troubleshooting performance issues",
    "section": "",
    "text": "Performance is a big concern for the backend and its users. We explain some key concepts and ways to troubleshoot potential issues, allowing to identify the root cause.\n\n\nGiven the broad API provided by openEO, there are multiple types of issues that are often labeled ‘performance issue’. It is a good idea to be more specific about the type of issue you are facing, especially when communicating to the developers.\nThese are some common cases that you might encounter:\n\nBatch job latency: batch jobs are scheduled on processing resources, but take time to start, often due to many users running jobs at the same time.\nBatch job slow wall time: the job takes a relatively long time to complete, but the credit cost is low.\nBatch job high credit cost: the job costs more than expected\nSlow result download\nSlow API responses for common operations like job listings\nSlow /execute requests (synchronous processing)\n\nOf these cases, (3) and (6) are the ones that we are actually discussing further. Cases (1) and (2) can often be considered ‘by design’ and depend more on the capacity of underlying cloud resources, which is tracked by the backend provider. The other cases may be signs of instability, but can also be caused by things like slow network connections.\nThe difference between case 2 and 3 is the concept of ‘wall time’ versus ‘total cpu time’: the wall time on the clock is what many people use to judge performance, but a bad indicator, because a job could be doing nothing at all for a lot of the time. The total cpu time is the best indicator of performance, because it counts time spent on all workers.\n\n\n\n\n\nOpenEO supports reading a huge variety of input data, and very often this is the culprit of slow processing. In some cases, one inefficient ‘load_collection’/‘load_stac’ can dominate the cost of the entire job.\nTo help with identifying slow reading, specific log messages have been added:\nload_collection: data was loaded with an average speed of: 6.764 Megapixel per second.\nStage 18 produced 442.67 MB in 5 minutes - load_collection: read by input product.\nNote that when it says it took ‘5 minutes’, this is total cpu time spent by multiple workers, so the actual wall time was a lot less.\n\n\n\nThe openEO process graph needs to be subdivided into concrete tasks that can be executed on a distributed processing cluster. Spark is responsible for most of this, but it is controlled by the ‘partitioner’ that is configured on the datacube.\nWhen performing distributed processing, performance does not increase linearly by subdividing the work into smaller tasks, instead, it is governed by Amdahl’s law, and performance increase reaches a limit when the overhead of managing the tasks becomes too high.\nIn addition to that, in Spark, having a suboptimal partitioner can even result in unnecessary ‘shuffles’ of your data, which are bad for performance especially for larger datasets.\nPartitioner problems are mostly visible in the Spark UI, which is not available to end users.\nThere’s two things to watch out for in the Spark UI:\n\nhigh task counts, with task duration statistics that are not normally distributed (like a few long running tasks and huge amount of smaller ones)\nhigh stage counts, meaning potentially too many shuffles\n\nNote that you can often focus on the stages that have the longest total task time (this can be different from duration). These tasks times are printed in the user logs, allowing to easily identify the stages that are causing the most trouble.\n\n\n\nSometimes, the executors are not doing anything at all, and are simply allocated while the driver is doing work or waiting for something to happen. This is not ideal, because total cpu hours might be adding up (unless if Spark removes executors via dynamic allocation).\nThe Spark UI shows this, but also user logs show when the last stage was finished. If there’s big gaps when all stages have finished and it takes long before a new message is printed, then it makes sense to figure out what was happening there.",
    "crumbs": [
      "Performance Debugging"
    ]
  },
  {
    "objectID": "performance_debug.html#step-1-categorize-your-issue",
    "href": "performance_debug.html#step-1-categorize-your-issue",
    "title": "Troubleshooting performance issues",
    "section": "",
    "text": "Given the broad API provided by openEO, there are multiple types of issues that are often labeled ‘performance issue’. It is a good idea to be more specific about the type of issue you are facing, especially when communicating to the developers.\nThese are some common cases that you might encounter:\n\nBatch job latency: batch jobs are scheduled on processing resources, but take time to start, often due to many users running jobs at the same time.\nBatch job slow wall time: the job takes a relatively long time to complete, but the credit cost is low.\nBatch job high credit cost: the job costs more than expected\nSlow result download\nSlow API responses for common operations like job listings\nSlow /execute requests (synchronous processing)\n\nOf these cases, (3) and (6) are the ones that we are actually discussing further. Cases (1) and (2) can often be considered ‘by design’ and depend more on the capacity of underlying cloud resources, which is tracked by the backend provider. The other cases may be signs of instability, but can also be caused by things like slow network connections.\nThe difference between case 2 and 3 is the concept of ‘wall time’ versus ‘total cpu time’: the wall time on the clock is what many people use to judge performance, but a bad indicator, because a job could be doing nothing at all for a lot of the time. The total cpu time is the best indicator of performance, because it counts time spent on all workers.",
    "crumbs": [
      "Performance Debugging"
    ]
  },
  {
    "objectID": "performance_debug.html#step-2-identify-root-cause",
    "href": "performance_debug.html#step-2-identify-root-cause",
    "title": "Troubleshooting performance issues",
    "section": "",
    "text": "OpenEO supports reading a huge variety of input data, and very often this is the culprit of slow processing. In some cases, one inefficient ‘load_collection’/‘load_stac’ can dominate the cost of the entire job.\nTo help with identifying slow reading, specific log messages have been added:\nload_collection: data was loaded with an average speed of: 6.764 Megapixel per second.\nStage 18 produced 442.67 MB in 5 minutes - load_collection: read by input product.\nNote that when it says it took ‘5 minutes’, this is total cpu time spent by multiple workers, so the actual wall time was a lot less.\n\n\n\nThe openEO process graph needs to be subdivided into concrete tasks that can be executed on a distributed processing cluster. Spark is responsible for most of this, but it is controlled by the ‘partitioner’ that is configured on the datacube.\nWhen performing distributed processing, performance does not increase linearly by subdividing the work into smaller tasks, instead, it is governed by Amdahl’s law, and performance increase reaches a limit when the overhead of managing the tasks becomes too high.\nIn addition to that, in Spark, having a suboptimal partitioner can even result in unnecessary ‘shuffles’ of your data, which are bad for performance especially for larger datasets.\nPartitioner problems are mostly visible in the Spark UI, which is not available to end users.\nThere’s two things to watch out for in the Spark UI:\n\nhigh task counts, with task duration statistics that are not normally distributed (like a few long running tasks and huge amount of smaller ones)\nhigh stage counts, meaning potentially too many shuffles\n\nNote that you can often focus on the stages that have the longest total task time (this can be different from duration). These tasks times are printed in the user logs, allowing to easily identify the stages that are causing the most trouble.\n\n\n\nSometimes, the executors are not doing anything at all, and are simply allocated while the driver is doing work or waiting for something to happen. This is not ideal, because total cpu hours might be adding up (unless if Spark removes executors via dynamic allocation).\nThe Spark UI shows this, but also user logs show when the last stage was finished. If there’s big gaps when all stages have finished and it takes long before a new message is printed, then it makes sense to figure out what was happening there.",
    "crumbs": [
      "Performance Debugging"
    ]
  },
  {
    "objectID": "udf-deps.html",
    "href": "udf-deps.html",
    "title": "UDF dependency handling",
    "section": "",
    "text": "In 0.83.x, with #237, initial support was added for inline dependencies in Python UDFs as discussed at https://open-eo.github.io/openeo-python-client/udf.html#standard-for-declaring-python-udf-dependencies, for example:\n# /// script\n# dependencies = [\n#   \"geojson\",\n#   \"fancy-eo-library\",\n# ]\n# ///\n#\n# This openEO UDF script implements ...\n# based on the fancy-eo-library ... using geosjon data ...\n\nimport geojson\nimport fancyeo\n\ndef apply_datacube(cube: xarray.DataArray, context: dict) -&gt; xarray.DataArray:\n    ...\nThe approach here was to extract the dependencies, install them in the job work directory from the batch job driver process, and include that directory in the Python path of the executors so that these libraries are available during UDF execution.\n\n\nInstalling the dependencies from the driver into the job work directory didn’t always work properly on some deployments (e.g. CDSE) where the job work directory is backed by a FUSE-mounted S3 storage. The full file tree of the dependencies can be pretty big (lot of small files), which is not ideal in such contexts.\nWith #845 (0.46.x) an alternative approach was added: - on the driver - install dependencies in a temporary directory - create a ZIP archive (a single large file) of the dependencies in the job work directory - on the executors, when executing UDFs: - unzip the dependencies on-the-fly from the ZIP archive to a temporary directory - include that directory in the Python path\n\n\n\nTo support the different UDF dependency handling strategies, a config option udf_dependencies_install_mode was added, with options:\n\n“disabled” to not do any automatic UDF dependency handling\n“direct” to install dependencies directly in the job work directory. This is currently the default, as it was the existing behavior, but it’s likely that “disabled” will become the default in the future.\n“zip” to use the ZIP based UDF handling."
  },
  {
    "objectID": "udf-deps.html#automatic-python-udf-dependency-handling",
    "href": "udf-deps.html#automatic-python-udf-dependency-handling",
    "title": "UDF dependency handling",
    "section": "",
    "text": "In 0.83.x, with #237, initial support was added for inline dependencies in Python UDFs as discussed at https://open-eo.github.io/openeo-python-client/udf.html#standard-for-declaring-python-udf-dependencies, for example:\n# /// script\n# dependencies = [\n#   \"geojson\",\n#   \"fancy-eo-library\",\n# ]\n# ///\n#\n# This openEO UDF script implements ...\n# based on the fancy-eo-library ... using geosjon data ...\n\nimport geojson\nimport fancyeo\n\ndef apply_datacube(cube: xarray.DataArray, context: dict) -&gt; xarray.DataArray:\n    ...\nThe approach here was to extract the dependencies, install them in the job work directory from the batch job driver process, and include that directory in the Python path of the executors so that these libraries are available during UDF execution.\n\n\nInstalling the dependencies from the driver into the job work directory didn’t always work properly on some deployments (e.g. CDSE) where the job work directory is backed by a FUSE-mounted S3 storage. The full file tree of the dependencies can be pretty big (lot of small files), which is not ideal in such contexts.\nWith #845 (0.46.x) an alternative approach was added: - on the driver - install dependencies in a temporary directory - create a ZIP archive (a single large file) of the dependencies in the job work directory - on the executors, when executing UDFs: - unzip the dependencies on-the-fly from the ZIP archive to a temporary directory - include that directory in the Python path\n\n\n\nTo support the different UDF dependency handling strategies, a config option udf_dependencies_install_mode was added, with options:\n\n“disabled” to not do any automatic UDF dependency handling\n“direct” to install dependencies directly in the job work directory. This is currently the default, as it was the existing behavior, but it’s likely that “disabled” will become the default in the future.\n“zip” to use the ZIP based UDF handling."
  },
  {
    "objectID": "geotrellis_processing.html",
    "href": "geotrellis_processing.html",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "This page describes some implementation details about the openEO processing based on Geotrellis.\n\n\nThe core of the implementation is written in Scala, and can be found in the openeo-geotrellis-extensions repository.\nThe following image shows the mapping between key openEO concepts, and classes in the implementation:\n\n\n\nopenEO Geotrellis mapping\n\n\n\n\n\nBy design, the Geotrellis implementation relies on distributed processing via the Apache Spark library. Spark gives us a framework to describe and execute distributed processing workflows that transform arbitrary collections of objects. In Spark, these are called resilient distributed datasets, or RDDs.\nGeotrellis is a framework that helps with representing georeferenced raster and vector data and also supports Spark. Hence, the objects in our Spark collections (RDD), are key-value pairs where the key contains a timestamp and a row and column in a spatial grid. The value is multiband raster tile. Such a tile contains chunks of a fixed size, by default 256x256 pixels, for each band in the openEO data cube.\nSo let’s take the example of an ‘apply’ process in openEO, with the process set to ‘abs’. In this case, Spark can simply apply the absolute value process to each 2D tile in parallel. If a chain of processes is applied, Spark and Geotrellis functionality is used to make sure that this all happens as efficiently as possible.\nAn important feature of Spark is that intermediate results are kept in memory whenever possible and only spilled to disk when needed. Writing to disk is entirely hidden for the actual processing workflow. This is quite different from more traditional batch processing workflows that commonly write a lot of intermediate results to disk.\nOther aspects covered by the combination of Spark and Geotrellis are complex operations such as merging data cubes, resampling or applying functions over the complete timeseries for a given pixel. In these cases, it is often needed to reorganize the the dataset in various ways. The Geotrellis openEO implementation tries to optimize all these cases.\nFor UDF’s, where user-provided Python code is executed, we similarly reorganize the dataset, depending on what spatiotemporal subset of the datacube can be processed in parallel. Then the data is converted into a Python XArray object which is passed on to the user code for transformation.\n\n\n\nThe main openEO concept implemented by this library is the Raster data cube, which is a multi-dimensional array of raster data.\nThis library does not support arbitrary multidimensional cubes, but rather focuses on the type of cubes that are most commonly encountered in earth observation. The most complex type that we can represent is the 4-dimensional space-time-bands cube.\nThis cube is mapped to a Geotrellis MultibandTileLayerRDD\nFor cubes with a temporal dimension, a SpacetimeKey is used, while spatial cubes use the SpatialKey.\nGeotrellis documents this class and its associated metadata here.\nThe key class describing the spatial layout of a cube is the LayoutDefinition, which combines an arbitrary spatial extent with a regular tile grid (TileLayout). This also defines the pixel size, which is fixed for the entire cube.\n\n\n\nLayout Definition\n\n\nTo be able to understand how these cubes work and behave, or how to correctly manipulate them, knowledge of the Spark RDD concept is essential. These are distributed datasets that (in production) live on multiple machines in a cluster, allowing openEO to effectively scale out computations.\n\n\nLoading large EO datasets into a Spark RDD is complex, mainly due to the variety in data formats, but also because IO is often a performance & stability bottleneck. Depending on the characteristics of the underlying storage system, the optimal loading strategy may also differ, which is why we support multiple code paths.\nThe main class for loading data from POSIX or Object Storage type of systems is the FileLayerProvider.\nThis load_collection implementation supports a number of features:\n\nResampling at load time, improving both performance and correctness.\nLoading of GDAL supported formats\nLoading of Geotiff using a native implementation\nApplying masks at load time, reducing the amount of data to read\nApplying polygonal filtering at load time\n\n\n\n\nAs mentioned, Spark will distribute data cubes over a cluster, into groups of data called partitions. A partition needs to fit into memory for processing, so the size matters, and Spark is not able to know the optimal partitioning for any given problem.\nDepending on procesess that are used, the optimal partitioning scheme can also change: for time series analysis, it would be optimal to group data for the same location with multiple observations in the same group. For other processes, like resample_spatial, it may be needed to have information from neighbouring tiles, so a grouping per observation would be more optimal. As a rule of thumb it is up to the process to check the partitioner, and change it if needed.\nWhenever a partitioner is changed, the data will be shuffled, which is a costly operation. This is why the code often tries to cleverly avoid this where possible.\nAssigning keys to partitions happens based on an indexing scheme. Usually, it is recommended to consider the spatiotemporal nature of the data to select the optimal scheme. Geotrellis explains a few potential schemes here.\nThe default scheme in openEO is the Z-curve. The 2D variant is shown below, for cubes with a time dimension, the 3D variant is used.\n\n\n\nZ-curve\n\n\n\n\nData cubes can be sparse because openEO supports operations on polygons, that are not necessarily spatially adjacent. Examples include the use of aggregate_spatial on a vector cube or the filter_spatial process on a vector cube.\nWhen this case occurs, regular Geotrellis spatial partitioners tend to create too many partitions, because they consider the full bounding box instead of the more detailed geometry locations. The same can occur for data which is irregular in the temporal dimension.\nThe effect of having too many partitions, is huge numbers of Spark tasks that do not do anything, but still consume resources as they are nevertheless scheduled. This becomes especially noticeable when the actual work that needs to be done is small.\nSparse partitioners avoid this problem by determining all of the SpacetimeKeys up front. We also store the list of keys in the partitioner itself, allowing certain operations to be implemented more efficiently.\n\n\n\n\n\nThis library implements a large part of the openEO process specification, mainly by using Geotrellis functionality.\nDepending on the type of process, the implementations can be found in 2 main locations: OpenEOProcesses and OpenEOProcessScriptBuilder.\nOpenEOProcesses implements the Scala part of predefined processes that operate on full data cubes. This Scala code is usually invoked by a Python counterpart in GeopysparkDataCube. Some of this Scala code may be used by multiple openEO process implementations. For instance, openEO reduce_dimension and apply_dimension can in some cases use the same code.\nOpenEOProcessScriptBuilder supports openEO processes that operate on arrays of numbers, which are often encountered in openEO child processes, as explained here. This part of the work is not distributed on Spark, so operates on chunks of data that fit in memory. It does use Geotrellis which generally has quite well performing implementations for most basic processes.\n\n\n\n\n\nA Geotrellis cube has fixed dimensions, in this order: time, y, x, and bands. This does not prevent us from advertising certain operations on dimensions at the metadata level, but does imply some limitations. In practice, this limitation has little impact on most real-world use cases.\n\n\n\nBands are represented by a single array, and thus needs to fit in memory. Some applications require more than 100 bands, and thus either need to increase memory, or decrease the chunk size.\n\n\n\n\nThe Geotrellis implementation has a number of tunables that can be used to configure a specific backend for an environment, or that can be set per batch job. Most of them can be found in openeogeotrellis.config.config.GpsBackendConfig.\n\ntile_size is the size in pixels of a single tile, as returned by load_collection. Large tiles benefit operations on spatial neighborhoods, but may require memory or cause jobs to fail. Small tiles may create more overhead from parallellization or task scheduling.\nexecutor_threads_jvm is the number of worker threads given to the Java/Scala part of the processing. This only helps if operations effectively use parallel processing. In specific combinations it can cause crashes due to threading issues.\nGDAL has various caching related settings that may impact IO performance, but also increase memory usage throughout the lifetime of the job.\n\n\n\n\nThe behaviour and configuration of an openEO backend heavily depends on the performance characteristics of the environment in which it is deployed.\nA very common one is a ‘cloud’ environment, which we define as having the following characteristics:\n\nA cpu to memory ratio of about 4GB per cpu, where requesting more memory is possible but will increase costs, while reducing memory is also possible, but may not necessarily free up the cpu, even though overcommitting cpu is possible.\nObject storage access for data, with relatively high latency per request, but good throughput. Supports many requests in parallel.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#overview",
    "href": "geotrellis_processing.html#overview",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "The core of the implementation is written in Scala, and can be found in the openeo-geotrellis-extensions repository.\nThe following image shows the mapping between key openEO concepts, and classes in the implementation:\n\n\n\nopenEO Geotrellis mapping",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#general-processing-strategy",
    "href": "geotrellis_processing.html#general-processing-strategy",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "By design, the Geotrellis implementation relies on distributed processing via the Apache Spark library. Spark gives us a framework to describe and execute distributed processing workflows that transform arbitrary collections of objects. In Spark, these are called resilient distributed datasets, or RDDs.\nGeotrellis is a framework that helps with representing georeferenced raster and vector data and also supports Spark. Hence, the objects in our Spark collections (RDD), are key-value pairs where the key contains a timestamp and a row and column in a spatial grid. The value is multiband raster tile. Such a tile contains chunks of a fixed size, by default 256x256 pixels, for each band in the openEO data cube.\nSo let’s take the example of an ‘apply’ process in openEO, with the process set to ‘abs’. In this case, Spark can simply apply the absolute value process to each 2D tile in parallel. If a chain of processes is applied, Spark and Geotrellis functionality is used to make sure that this all happens as efficiently as possible.\nAn important feature of Spark is that intermediate results are kept in memory whenever possible and only spilled to disk when needed. Writing to disk is entirely hidden for the actual processing workflow. This is quite different from more traditional batch processing workflows that commonly write a lot of intermediate results to disk.\nOther aspects covered by the combination of Spark and Geotrellis are complex operations such as merging data cubes, resampling or applying functions over the complete timeseries for a given pixel. In these cases, it is often needed to reorganize the the dataset in various ways. The Geotrellis openEO implementation tries to optimize all these cases.\nFor UDF’s, where user-provided Python code is executed, we similarly reorganize the dataset, depending on what spatiotemporal subset of the datacube can be processed in parallel. Then the data is converted into a Python XArray object which is passed on to the user code for transformation.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#data-cubes",
    "href": "geotrellis_processing.html#data-cubes",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "The main openEO concept implemented by this library is the Raster data cube, which is a multi-dimensional array of raster data.\nThis library does not support arbitrary multidimensional cubes, but rather focuses on the type of cubes that are most commonly encountered in earth observation. The most complex type that we can represent is the 4-dimensional space-time-bands cube.\nThis cube is mapped to a Geotrellis MultibandTileLayerRDD\nFor cubes with a temporal dimension, a SpacetimeKey is used, while spatial cubes use the SpatialKey.\nGeotrellis documents this class and its associated metadata here.\nThe key class describing the spatial layout of a cube is the LayoutDefinition, which combines an arbitrary spatial extent with a regular tile grid (TileLayout). This also defines the pixel size, which is fixed for the entire cube.\n\n\n\nLayout Definition\n\n\nTo be able to understand how these cubes work and behave, or how to correctly manipulate them, knowledge of the Spark RDD concept is essential. These are distributed datasets that (in production) live on multiple machines in a cluster, allowing openEO to effectively scale out computations.\n\n\nLoading large EO datasets into a Spark RDD is complex, mainly due to the variety in data formats, but also because IO is often a performance & stability bottleneck. Depending on the characteristics of the underlying storage system, the optimal loading strategy may also differ, which is why we support multiple code paths.\nThe main class for loading data from POSIX or Object Storage type of systems is the FileLayerProvider.\nThis load_collection implementation supports a number of features:\n\nResampling at load time, improving both performance and correctness.\nLoading of GDAL supported formats\nLoading of Geotiff using a native implementation\nApplying masks at load time, reducing the amount of data to read\nApplying polygonal filtering at load time\n\n\n\n\nAs mentioned, Spark will distribute data cubes over a cluster, into groups of data called partitions. A partition needs to fit into memory for processing, so the size matters, and Spark is not able to know the optimal partitioning for any given problem.\nDepending on procesess that are used, the optimal partitioning scheme can also change: for time series analysis, it would be optimal to group data for the same location with multiple observations in the same group. For other processes, like resample_spatial, it may be needed to have information from neighbouring tiles, so a grouping per observation would be more optimal. As a rule of thumb it is up to the process to check the partitioner, and change it if needed.\nWhenever a partitioner is changed, the data will be shuffled, which is a costly operation. This is why the code often tries to cleverly avoid this where possible.\nAssigning keys to partitions happens based on an indexing scheme. Usually, it is recommended to consider the spatiotemporal nature of the data to select the optimal scheme. Geotrellis explains a few potential schemes here.\nThe default scheme in openEO is the Z-curve. The 2D variant is shown below, for cubes with a time dimension, the 3D variant is used.\n\n\n\nZ-curve\n\n\n\n\nData cubes can be sparse because openEO supports operations on polygons, that are not necessarily spatially adjacent. Examples include the use of aggregate_spatial on a vector cube or the filter_spatial process on a vector cube.\nWhen this case occurs, regular Geotrellis spatial partitioners tend to create too many partitions, because they consider the full bounding box instead of the more detailed geometry locations. The same can occur for data which is irregular in the temporal dimension.\nThe effect of having too many partitions, is huge numbers of Spark tasks that do not do anything, but still consume resources as they are nevertheless scheduled. This becomes especially noticeable when the actual work that needs to be done is small.\nSparse partitioners avoid this problem by determining all of the SpacetimeKeys up front. We also store the list of keys in the partitioner itself, allowing certain operations to be implemented more efficiently.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#openeo-processes",
    "href": "geotrellis_processing.html#openeo-processes",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "This library implements a large part of the openEO process specification, mainly by using Geotrellis functionality.\nDepending on the type of process, the implementations can be found in 2 main locations: OpenEOProcesses and OpenEOProcessScriptBuilder.\nOpenEOProcesses implements the Scala part of predefined processes that operate on full data cubes. This Scala code is usually invoked by a Python counterpart in GeopysparkDataCube. Some of this Scala code may be used by multiple openEO process implementations. For instance, openEO reduce_dimension and apply_dimension can in some cases use the same code.\nOpenEOProcessScriptBuilder supports openEO processes that operate on arrays of numbers, which are often encountered in openEO child processes, as explained here. This part of the work is not distributed on Spark, so operates on chunks of data that fit in memory. It does use Geotrellis which generally has quite well performing implementations for most basic processes.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#specific-properties-of-the-geotrellis-implementation",
    "href": "geotrellis_processing.html#specific-properties-of-the-geotrellis-implementation",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "A Geotrellis cube has fixed dimensions, in this order: time, y, x, and bands. This does not prevent us from advertising certain operations on dimensions at the metadata level, but does imply some limitations. In practice, this limitation has little impact on most real-world use cases.\n\n\n\nBands are represented by a single array, and thus needs to fit in memory. Some applications require more than 100 bands, and thus either need to increase memory, or decrease the chunk size.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#tunables",
    "href": "geotrellis_processing.html#tunables",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "The Geotrellis implementation has a number of tunables that can be used to configure a specific backend for an environment, or that can be set per batch job. Most of them can be found in openeogeotrellis.config.config.GpsBackendConfig.\n\ntile_size is the size in pixels of a single tile, as returned by load_collection. Large tiles benefit operations on spatial neighborhoods, but may require memory or cause jobs to fail. Small tiles may create more overhead from parallellization or task scheduling.\nexecutor_threads_jvm is the number of worker threads given to the Java/Scala part of the processing. This only helps if operations effectively use parallel processing. In specific combinations it can cause crashes due to threading issues.\nGDAL has various caching related settings that may impact IO performance, but also increase memory usage throughout the lifetime of the job.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "geotrellis_processing.html#runtime-properties-of-a-typical-cloud-setup",
    "href": "geotrellis_processing.html#runtime-properties-of-a-typical-cloud-setup",
    "title": "Geotrellis openEO processing",
    "section": "",
    "text": "The behaviour and configuration of an openEO backend heavily depends on the performance characteristics of the environment in which it is deployed.\nA very common one is a ‘cloud’ environment, which we define as having the following characteristics:\n\nA cpu to memory ratio of about 4GB per cpu, where requesting more memory is possible but will increase costs, while reducing memory is also possible, but may not necessarily free up the cpu, even though overcommitting cpu is possible.\nObject storage access for data, with relatively high latency per request, but good throughput. Supports many requests in parallel.",
    "crumbs": [
      "Geotrellis Processing"
    ]
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Development",
    "section": "",
    "text": "As usually recommended in modern Python development, it is best to use some sort of virtual environment to sandbox this application and its dependencies.\nopenEO GeoPySpark depends on openeo-python-driver in a “bleeding edge” fashion as both projects are developed in tandem. It is therefor recommended to install both projects from source in the same virtual environment. As usual in this kind of development setup, we will install both projects in “editable” mode (pip option -e) so that changes to the source code are immediately reflected in the virtual environment.\nIn addition, one can also choose to install the openeo Python client project the same way in the virtual environment, if one is planning to have this project in development scope as well.\n\n\nIn your development workspace, clone the projects, e.g. as follows:\ngit clone --recursive git@github.com:Open-EO/openeo-geopyspark-driver.git\ngit clone --recursive git@github.com:Open-EO/openeo-python-driver.git\n# Optionally:\ngit clone git@github.com:Open-EO/openeo-python-client.git\n\n\n\nWe’ll create (and acticate) a venv in the openeo-geopyspark-driver directory. As noted, you might want to use a different virtual environment tool as desired.\ncd openeo-geopyspark-driver\npython -m venv --prompt . venv\nsource venv/bin/activate\n\n\n\n# Optional: install the openeo-python-client project from source:\npip install -e ../openeo-python-client\n\n# First install the openeo-python-driver project (with \"dev\" extra):\npip install -e ../openeo-python-driver[dev]\n\n# Finally install the openeo-geopyspark-driver project itself (with \"dev\" extra):\npip install -e .[dev] --extra-index-url https://artifactory.vgt.vito.be/artifactory/api/pypi/python-openeo/simple\n\n\n\n\nThe openEO GeoPySpark driver also depends on some JAR files from openeo-geotrellis-extensions. these can be fetched with the following script:\npython scripts/get-jars.py\n\n\n\nYou can run the service with:\nexport SPARK_HOME=$(find_spark_home.py)\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport FLASK_DEBUG=1\npython openeogeotrellis/deploy/local.py\n\n\nThis will set up the environment (Java/Python with dependencies) and makes it possible to run simple process graphs. For more information, see: docker/local_batch_job/",
    "crumbs": [
      "Development"
    ]
  },
  {
    "objectID": "development.html#virtual-environment-setup",
    "href": "development.html#virtual-environment-setup",
    "title": "Development",
    "section": "",
    "text": "As usually recommended in modern Python development, it is best to use some sort of virtual environment to sandbox this application and its dependencies.\nopenEO GeoPySpark depends on openeo-python-driver in a “bleeding edge” fashion as both projects are developed in tandem. It is therefor recommended to install both projects from source in the same virtual environment. As usual in this kind of development setup, we will install both projects in “editable” mode (pip option -e) so that changes to the source code are immediately reflected in the virtual environment.\nIn addition, one can also choose to install the openeo Python client project the same way in the virtual environment, if one is planning to have this project in development scope as well.\n\n\nIn your development workspace, clone the projects, e.g. as follows:\ngit clone --recursive git@github.com:Open-EO/openeo-geopyspark-driver.git\ngit clone --recursive git@github.com:Open-EO/openeo-python-driver.git\n# Optionally:\ngit clone git@github.com:Open-EO/openeo-python-client.git\n\n\n\nWe’ll create (and acticate) a venv in the openeo-geopyspark-driver directory. As noted, you might want to use a different virtual environment tool as desired.\ncd openeo-geopyspark-driver\npython -m venv --prompt . venv\nsource venv/bin/activate\n\n\n\n# Optional: install the openeo-python-client project from source:\npip install -e ../openeo-python-client\n\n# First install the openeo-python-driver project (with \"dev\" extra):\npip install -e ../openeo-python-driver[dev]\n\n# Finally install the openeo-geopyspark-driver project itself (with \"dev\" extra):\npip install -e .[dev] --extra-index-url https://artifactory.vgt.vito.be/artifactory/api/pypi/python-openeo/simple",
    "crumbs": [
      "Development"
    ]
  },
  {
    "objectID": "development.html#additional-dependencies",
    "href": "development.html#additional-dependencies",
    "title": "Development",
    "section": "",
    "text": "The openEO GeoPySpark driver also depends on some JAR files from openeo-geotrellis-extensions. these can be fetched with the following script:\npython scripts/get-jars.py",
    "crumbs": [
      "Development"
    ]
  },
  {
    "objectID": "development.html#running-locally",
    "href": "development.html#running-locally",
    "title": "Development",
    "section": "",
    "text": "You can run the service with:\nexport SPARK_HOME=$(find_spark_home.py)\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport FLASK_DEBUG=1\npython openeogeotrellis/deploy/local.py\n\n\nThis will set up the environment (Java/Python with dependencies) and makes it possible to run simple process graphs. For more information, see: docker/local_batch_job/",
    "crumbs": [
      "Development"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "(Unit) testing",
    "section": "",
    "text": "The unit tests expect that environment variable SPARK_HOME is set, which can easily be done from within your development virtual environment as follows:\nexport SPARK_HOME=$(find_spark_home.py)\npytest\n\n\n\nRun specific test or subset of test: use -k option, e.g. run all tests with “udp” in function name:\npytest -k udp\nTip: The general setup in conftest.py spins up a local Spark instance (see pytest_configure hook) as some tests need a working Spark cluster. Setting this local Spark instance takes a bit of time, which can be annoying during development if you repeatedly are running some subset of tests that don’t require Spark in any way. You can disable setting up Spark for these runs with this env var:\nOPENEO_TESTING_SETUP_SPARK=no\n(Don’t forget to unset this when running the full test suite or tests that do require Spark)\n\n\n\nTo disable capturing of print and logging, use something like this:\n    pytest --capture=no --log-cli-level=INFO"
  },
  {
    "objectID": "testing.html#basics",
    "href": "testing.html#basics",
    "title": "(Unit) testing",
    "section": "",
    "text": "The unit tests expect that environment variable SPARK_HOME is set, which can easily be done from within your development virtual environment as follows:\nexport SPARK_HOME=$(find_spark_home.py)\npytest"
  },
  {
    "objectID": "testing.html#selection-and-filtering",
    "href": "testing.html#selection-and-filtering",
    "title": "(Unit) testing",
    "section": "",
    "text": "Run specific test or subset of test: use -k option, e.g. run all tests with “udp” in function name:\npytest -k udp\nTip: The general setup in conftest.py spins up a local Spark instance (see pytest_configure hook) as some tests need a working Spark cluster. Setting this local Spark instance takes a bit of time, which can be annoying during development if you repeatedly are running some subset of tests that don’t require Spark in any way. You can disable setting up Spark for these runs with this env var:\nOPENEO_TESTING_SETUP_SPARK=no\n(Don’t forget to unset this when running the full test suite or tests that do require Spark)"
  },
  {
    "objectID": "testing.html#various-tips-and-tricks",
    "href": "testing.html#various-tips-and-tricks",
    "title": "(Unit) testing",
    "section": "",
    "text": "To disable capturing of print and logging, use something like this:\n    pytest --capture=no --log-cli-level=INFO"
  },
  {
    "objectID": "vectorcube-run_udf.html",
    "href": "vectorcube-run_udf.html",
    "title": "Using run_udf on a vector cube data cube",
    "section": "",
    "text": "The openEO process run_udf is typically used on raster data cubes in the “callback” of a process like reduce_dimension, apply_dimension, … where the UDF operates on a slice of raster data, provided in some kind of multidimensional array format (like numpy, Xarray, pandas, …).\nThe VITO/Terrascope openEO back-end also adds experimental support to use run_udf directly on a vector cube, e.g. to filter, transform, enrich, postprocess the vector data. In the original implementation (which is still the default), the back-end calls the UDF with the whole vector data set as input. This was fine as proof of concept, but did not scale well for large vector cubes as there was no way to leverage parallelization.\n\n\nUnder Open-EO/openeo-geopyspark-driver#251, the experimental run_udf support on vector cubes was further expanded to allow parallelized execution of the UDF logic.\nThe user-provided UDF is expected to work at the level of single geometries and must follow the UDF signatures described below.\nIn the examples below, we will assume to apply the run_udf process on the result of the aggregate_spatial process. For example, something like this:\nimport openeo\nimport openeo.processes\n...\ncube = connection.load_collection(...)\naggregates = cube.aggregate_spatial(geometries, reducer=\"mean\")\n\nresult = openeo.processes.run_udf(data=aggregates, udf=udf_code, runtime=\"Python\")\nThe resulting dataframe structure (to be downloaded in JSON format) is currently structured according to the “split” mode of pandas.DataFrame.to_dict For example, when using synchronous execution/download:\n&gt;&gt;&gt; data = result.execute()\n&gt;&gt;&gt; data\n{\n    \"columns\": [\"feature_index\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n    \"data\": [\n        [0, 0.4, 0.8, 1.4],\n        [1, 0.3, 0.6, 1.9],\n        ...\n\n\nThis mode can be enabled by defining your UDF entry point function as udf_apply_feature_dataframe, which will be given a pandas DataFrame, containing the data of a single geometry/feature in your vector cube:\nimport pandas as pd\n\ndef udf_apply_feature_dataframe(df: pd.DataFrame):\n    # df contains data for a single geometry/feature\n    # with time dimension as index, and band dimension as columns\n    ...\nDepending on your use case, you can return different values:\n\nreturn a scalar (reduce all data of a feature to a single value), for example (extremely simplified example):\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; float:\n  return 123.456\nThe resulting output data structure will list the returned scalar value for each geometry/feature, e.g.:\n{\n  \"columns\": [\"feature_index\", \"0\"],\n  \"data\": [\n    [0, 123.456],\n    [1, 123.456],\n    ...\n  ],\n  ...\n}\nreturn a pandas Series to:\n\nreduce the time dimension:\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.Series:\n    # Sum along index (time dimension)\n    return df.sum(axis=0)\nreduce the band dimension (make sure to convert the time index labels to strings):\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.Series:\n    # Sum along columns (band dimension)\n    series = df.sum(axis=0)\n    # Make sure index labels are strings\n    series.index = series.index.strftime(\"%Y-%m-%d\")\n    return series\n\nThe resulting output data structure will list the calculated values per geometry as follows:\n{\n  \"columns\": [\"feature_index\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n  \"data\": [\n    [0, 0.4, 0.8, 1.4],\n    [1, 1.3, 0.3, 2.3],\n    ...\n  ],\n  ...\n}\nreturn a full pandas DataFrame, for example (very simplified example)\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df + 1000\nThe resulting output data structure will encode the preserved time dimension and band dimension as follows:\n{\n  \"columns\": [\"feature_index\", \"date\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n  \"data\": [\n    [0, \"2021-01-05T00:00:00.000Z\", 1000.4, 1000.8, 1001.4],\n    [0, \"2021-02-12T00:00:00.000Z\", 1000.8, 1002.8, 1000.9],\n    [1, \"2021-01-05T00:00:00.000Z\", 1001.8, 1000.3, 1002.7],\n    [1, \"2021-01-12T00:00:00.000Z\", 1000.3, 1000.9, 1001.6],\n    ...\n  ],\n  ...\n}\n\n\n\n\nThis mode is more cumbersome to work with, because there is more unpacking and packing boilerplate code necessary.\nSee the unit tests for this mode for more information:\nhttps://github.com/Open-EO/openeo-geopyspark-driver/blob/84c9c349159916037ea46c7982fe98143d8c40ee/tests/test_api_result.py#L1907-L2092",
    "crumbs": [
      "Running UDF on vector cube"
    ]
  },
  {
    "objectID": "vectorcube-run_udf.html#parallelized-run_udf-on-vector-cube-data",
    "href": "vectorcube-run_udf.html#parallelized-run_udf-on-vector-cube-data",
    "title": "Using run_udf on a vector cube data cube",
    "section": "",
    "text": "Under Open-EO/openeo-geopyspark-driver#251, the experimental run_udf support on vector cubes was further expanded to allow parallelized execution of the UDF logic.\nThe user-provided UDF is expected to work at the level of single geometries and must follow the UDF signatures described below.\nIn the examples below, we will assume to apply the run_udf process on the result of the aggregate_spatial process. For example, something like this:\nimport openeo\nimport openeo.processes\n...\ncube = connection.load_collection(...)\naggregates = cube.aggregate_spatial(geometries, reducer=\"mean\")\n\nresult = openeo.processes.run_udf(data=aggregates, udf=udf_code, runtime=\"Python\")\nThe resulting dataframe structure (to be downloaded in JSON format) is currently structured according to the “split” mode of pandas.DataFrame.to_dict For example, when using synchronous execution/download:\n&gt;&gt;&gt; data = result.execute()\n&gt;&gt;&gt; data\n{\n    \"columns\": [\"feature_index\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n    \"data\": [\n        [0, 0.4, 0.8, 1.4],\n        [1, 0.3, 0.6, 1.9],\n        ...\n\n\nThis mode can be enabled by defining your UDF entry point function as udf_apply_feature_dataframe, which will be given a pandas DataFrame, containing the data of a single geometry/feature in your vector cube:\nimport pandas as pd\n\ndef udf_apply_feature_dataframe(df: pd.DataFrame):\n    # df contains data for a single geometry/feature\n    # with time dimension as index, and band dimension as columns\n    ...\nDepending on your use case, you can return different values:\n\nreturn a scalar (reduce all data of a feature to a single value), for example (extremely simplified example):\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; float:\n  return 123.456\nThe resulting output data structure will list the returned scalar value for each geometry/feature, e.g.:\n{\n  \"columns\": [\"feature_index\", \"0\"],\n  \"data\": [\n    [0, 123.456],\n    [1, 123.456],\n    ...\n  ],\n  ...\n}\nreturn a pandas Series to:\n\nreduce the time dimension:\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.Series:\n    # Sum along index (time dimension)\n    return df.sum(axis=0)\nreduce the band dimension (make sure to convert the time index labels to strings):\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.Series:\n    # Sum along columns (band dimension)\n    series = df.sum(axis=0)\n    # Make sure index labels are strings\n    series.index = series.index.strftime(\"%Y-%m-%d\")\n    return series\n\nThe resulting output data structure will list the calculated values per geometry as follows:\n{\n  \"columns\": [\"feature_index\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n  \"data\": [\n    [0, 0.4, 0.8, 1.4],\n    [1, 1.3, 0.3, 2.3],\n    ...\n  ],\n  ...\n}\nreturn a full pandas DataFrame, for example (very simplified example)\ndef udf_apply_feature_dataframe(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df + 1000\nThe resulting output data structure will encode the preserved time dimension and band dimension as follows:\n{\n  \"columns\": [\"feature_index\", \"date\", \"mean(band_0)\", \"mean(band_1)\", \"mean(band_2)\"],\n  \"data\": [\n    [0, \"2021-01-05T00:00:00.000Z\", 1000.4, 1000.8, 1001.4],\n    [0, \"2021-02-12T00:00:00.000Z\", 1000.8, 1002.8, 1000.9],\n    [1, \"2021-01-05T00:00:00.000Z\", 1001.8, 1000.3, 1002.7],\n    [1, \"2021-01-12T00:00:00.000Z\", 1000.3, 1000.9, 1001.6],\n    ...\n  ],\n  ...\n}\n\n\n\n\nThis mode is more cumbersome to work with, because there is more unpacking and packing boilerplate code necessary.\nSee the unit tests for this mode for more information:\nhttps://github.com/Open-EO/openeo-geopyspark-driver/blob/84c9c349159916037ea46c7982fe98143d8c40ee/tests/test_api_result.py#L1907-L2092",
    "crumbs": [
      "Running UDF on vector cube"
    ]
  },
  {
    "objectID": "job_options.html",
    "href": "job_options.html",
    "title": "Tweak batch jobs with job_options",
    "section": "",
    "text": "Tweak batch jobs with job_options\nThe VITO/Terrascope openEO back-end supports passing additional options at job creation time to tweak batch jobs:\nconnection.create_job(process_graph, additional={\n    \"driver-memory\": \"8G\",\n    \"driver-memoryOverhead\": \"2G\"\n})\ndata_cube.create_job(job_options={\n    \"sentinel-hub\": {\n        \"client-alias\": \"vito\",\n        \"input\": \"sync\"\n    }\n})\n\nLog level\nTo reduce: * the amount of irrelevant logs for users to wade through, * the amount of storage for these logs on disk,\nthe openEO API spec provides a log_level to set at job creation time, for example with create_job() or execute_batch():\ncube.create_job(\n    log_level=\"warning\",\n)\n(Note that, originally, this log level could also be set through a “logging-threshold” job option, but that usage pattern is deprecated in favor of the standardized way mentioned above)\nThis log level effectively prevents log entries with a lower level (in this case: “debug” and “info”) from being written to the logs.\nIt accepts the log levels as defined in the OpenEO API: “debug”, “info”, “warning” and “error” and defaults to “info”.\nNote: logs originating from openeo.udf.debug.inspect are unaffected by this threshold; regardless of their log level, be it “debug”, “info”, “warning” or “error”, they will always be written to the logs.\n\n\nAllow empty data cubes\nSetting the “allow_empty_cubes” flag (defaults to: false) enables working with empty data cubes where this otherwise would raise an error. It is applied to all collections within this job.\njob_options={\n    \"allow_empty_cubes\": True\n}"
  },
  {
    "objectID": "etl-organization-id.html",
    "href": "etl-organization-id.html",
    "title": "Organization Id Based ETL Reporting",
    "section": "",
    "text": "Organization Id Based ETL Reporting\nData processing in openEO (both batch jobs and synchronous processing) consumes credits. Advanced users can be member of multiple organizations, each of which is associated with a separate credit balance. Originally, users had to manually and globally switch the active organization under their account to make sure the correct organization’s credits were used for processing.\nWith #671 (openeo-geopyspark-driver version 0.53.0) a feature was added to select the desired organization for a specific job through the job options:\njob = cube.create_job(\n    ...,\n    job_options={\n        \"etl_organization_id\": 1234,\n    },\n)\nTo figure out the organization id (integer) of the desired organization:\n\nLog in to the marketplace portal corresponding to the openEO backend you are using, e.g.\n\nportal.terrascope.be for openEO Platform (openeo.cloud) or the Terrascope/VITO openEO backend (openeo.vito.be)\nmarketplace-portal.dataspace.copernicus.eu for the Copernicus Data Space Ecosystem (CDSE, openeo.dataspace.copernicus.eu or openeofed.dataspace.copernicus.eu)\n\nGo to your profile (URL path /profile) and switch to the desired organization under “Linked organisation” (make sure to press the “switch” button)\nGo to the “Organisation” tab (URL path /profile/organisation) and find the organization id, labeled “ID”, above the organization name. For example, 1234 in the following screenshot:"
  },
  {
    "objectID": "sar_backscatter_orfeo.html",
    "href": "sar_backscatter_orfeo.html",
    "title": "Orfeo toolbox sar_backscatter implementaton",
    "section": "",
    "text": "otbcli_OrthoRectification -io.in /eodata/Sentinel-1/SAR/IW_GRDH_1S-COG/2021/03/20/S1B_IW_GRDH_1SDV_20210320T052533_20210320T052553_026094_031D15_309A_COG.SAFE/measurement/s1b-iw-grd-vh-20210320t052533-20210320t052553-026094-03 1d15-002-cog.tiff -io.out /tmp/out.tiff -map epsg -map.epsg.code 3035 -outputs.spacingx 10.0 -outputs.spacingy -10.0 -opt.gridspacing 40 -outputs.ulx 4500000.0 -outputs.uly 2900000 -outputs.sizex 256 -outputs.sizey 256 -outputs.isotropi c true -elev.geoid /opt/openeo-vito-aux-data/egm96.tif\n\n\n\nfrom openeogeotrellis.collections.s1backscatter_orfeo import (\n    S1BackscatterOrfeo,\n    _instant_ms_to_day,\n    S1BackscatterOrfeoV2,\n)\nimport sys\nfrom pathlib import Path\n\n  \nX = 190300.0\nY = 2392840.0\nEXTENT = dict(zip([\"xmin\", \"ymin\", \"xmax\", \"ymax\"], [\n    X, Y, X + 10 * 15, Y + 10 * 15]))\nEXTENT[\"crs\"] = 32618\nEXTENT[\"srs\"] = 32618\n \ndata, nodata = S1BackscatterOrfeoV2._orfeo_pipeline(\n    input_tiff=Path(\n        \"/vsis3/eodata/Sentinel-1/SAR/IW_GRDH_1S-COG/2019/07/12/S1A_IW_GRDH_1SDV_20190712T111113_20190712T111130_028083_032BE7_E201_COG.SAFE/measurement/s1a-iw-grd-vh-20190712t111113-20190712t111130-028083-032be7-002-cog.tiff\"\n    ),\n    extent=EXTENT,\n    extent_epsg=3035,\n    dem_dir=None,\n    extent_width_px=1024,\n    extent_height_px=2048,\n    sar_calibration_lut=\"gamma\",\n    noise_removal=False,\n    elev_geoid=None,\n    elev_default=0,\n    log_prefix=\"test\",\n    orfeo_memory=1024\n)"
  },
  {
    "objectID": "sar_backscatter_orfeo.html#debugging",
    "href": "sar_backscatter_orfeo.html#debugging",
    "title": "Orfeo toolbox sar_backscatter implementaton",
    "section": "",
    "text": "otbcli_OrthoRectification -io.in /eodata/Sentinel-1/SAR/IW_GRDH_1S-COG/2021/03/20/S1B_IW_GRDH_1SDV_20210320T052533_20210320T052553_026094_031D15_309A_COG.SAFE/measurement/s1b-iw-grd-vh-20210320t052533-20210320t052553-026094-03 1d15-002-cog.tiff -io.out /tmp/out.tiff -map epsg -map.epsg.code 3035 -outputs.spacingx 10.0 -outputs.spacingy -10.0 -opt.gridspacing 40 -outputs.ulx 4500000.0 -outputs.uly 2900000 -outputs.sizex 256 -outputs.sizey 256 -outputs.isotropi c true -elev.geoid /opt/openeo-vito-aux-data/egm96.tif\n\n\n\nfrom openeogeotrellis.collections.s1backscatter_orfeo import (\n    S1BackscatterOrfeo,\n    _instant_ms_to_day,\n    S1BackscatterOrfeoV2,\n)\nimport sys\nfrom pathlib import Path\n\n  \nX = 190300.0\nY = 2392840.0\nEXTENT = dict(zip([\"xmin\", \"ymin\", \"xmax\", \"ymax\"], [\n    X, Y, X + 10 * 15, Y + 10 * 15]))\nEXTENT[\"crs\"] = 32618\nEXTENT[\"srs\"] = 32618\n \ndata, nodata = S1BackscatterOrfeoV2._orfeo_pipeline(\n    input_tiff=Path(\n        \"/vsis3/eodata/Sentinel-1/SAR/IW_GRDH_1S-COG/2019/07/12/S1A_IW_GRDH_1SDV_20190712T111113_20190712T111130_028083_032BE7_E201_COG.SAFE/measurement/s1a-iw-grd-vh-20190712t111113-20190712t111130-028083-032be7-002-cog.tiff\"\n    ),\n    extent=EXTENT,\n    extent_epsg=3035,\n    dem_dir=None,\n    extent_width_px=1024,\n    extent_height_px=2048,\n    sar_calibration_lut=\"gamma\",\n    noise_removal=False,\n    elev_geoid=None,\n    elev_default=0,\n    log_prefix=\"test\",\n    orfeo_memory=1024\n)"
  }
]